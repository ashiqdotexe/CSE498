{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pprint\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from cg_detr.config import BaseOptions\n",
    "from cg_detr.start_end_dataset import \\\n",
    "    StartEndDataset, start_end_collate, prepare_batch_inputs\n",
    "from cg_detr.inference import eval_epoch, start_inference, setup_model\n",
    "from utils.basic_utils import AverageMeter, dict_to_markdown\n",
    "from utils.model_utils import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s.%(msecs)03d:%(levelname)s:%(name)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, use_cuda=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, use_cuda=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def train_epoch(model, criterion, train_loader, optimizer, opt, epoch_i, tb_writer):\n",
    "    logger.info(f\"[Epoch {epoch_i+1}]\")\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "\n",
    "    # init meters\n",
    "    time_meters = defaultdict(AverageMeter)\n",
    "    loss_meters = defaultdict(AverageMeter)\n",
    "\n",
    "    num_training_examples = len(train_loader)\n",
    "    timer_dataloading = time.time()\n",
    "    for batch_idx, batch in tqdm(\n",
    "        enumerate(train_loader), desc=\"Training Iteration\", total=num_training_examples\n",
    "    ):\n",
    "        time_meters[\"dataloading_time\"].update(time.time() - timer_dataloading)\n",
    "\n",
    "        timer_start = time.time()\n",
    "\n",
    "        model_inputs, targets = prepare_batch_inputs(\n",
    "            batch[1], opt.device, non_blocking=opt.pin_memory\n",
    "        )\n",
    "\n",
    "        time_meters[\"prepare_inputs_time\"].update(time.time() - timer_start)\n",
    "        timer_start = time.time()\n",
    "\n",
    "        outputs = model(**model_inputs, targets=targets)\n",
    "        loss_dict = criterion(outputs, targets)\n",
    "        weight_dict = criterion.weight_dict\n",
    "        losses = sum(\n",
    "            loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict\n",
    "        )\n",
    "        time_meters[\"model_forward_time\"].update(time.time() - timer_start)\n",
    "\n",
    "        timer_start = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        if opt.grad_clip > 0:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip)\n",
    "        optimizer.step()\n",
    "        time_meters[\"model_backward_time\"].update(time.time() - timer_start)\n",
    "\n",
    "        loss_dict[\"loss_overall\"] = float(losses)  # for logging only\n",
    "        for k, v in loss_dict.items():\n",
    "            loss_meters[k].update(\n",
    "                float(v) * weight_dict[k] if k in weight_dict else float(v)\n",
    "            )\n",
    "\n",
    "        timer_dataloading = time.time()\n",
    "        if opt.debug and batch_idx == 3:\n",
    "            break\n",
    "\n",
    "    # print/add logs\n",
    "    tb_writer.add_scalar(\n",
    "        \"Train/lr\", float(optimizer.param_groups[0][\"lr\"]), epoch_i + 1\n",
    "    )\n",
    "    for k, v in loss_meters.items():\n",
    "        tb_writer.add_scalar(\"Train/{}\".format(k), v.avg, epoch_i + 1)\n",
    "\n",
    "    to_write = opt.train_log_txt_formatter.format(\n",
    "        time_str=time.strftime(\"%Y_%m_%d_%H_%M_%S\"),\n",
    "        epoch=epoch_i + 1,\n",
    "        loss_str=\" \".join(\n",
    "            [\"{} {:.4f}\".format(k, v.avg) for k, v in loss_meters.items()]\n",
    "        ),\n",
    "    )\n",
    "    with open(opt.train_log_filepath, \"a\") as f:\n",
    "        f.write(to_write)\n",
    "\n",
    "    logger.info(\"Epoch time stats:\")\n",
    "    for name, meter in time_meters.items():\n",
    "        d = {k: f\"{getattr(meter, k):.4f}\" for k in [\"max\", \"min\", \"avg\"]}\n",
    "        logger.info(f\"{name} ==> {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, lr_scheduler, train_dataset, val_dataset, opt):\n",
    "    if opt.device.type == \"cuda\":\n",
    "        logger.info(\"CUDA enabled.\")\n",
    "        model.to(opt.device)\n",
    "\n",
    "    tb_writer = SummaryWriter(opt.tensorboard_log_dir)\n",
    "    tb_writer.add_text(\"hyperparameters\", dict_to_markdown(vars(opt), max_str_len=None))\n",
    "    opt.train_log_txt_formatter = \"{time_str} [Epoch] {epoch:03d} [Loss] {loss_str}\\n\"\n",
    "    opt.eval_log_txt_formatter = \"{time_str} [Epoch] {epoch:03d} [Loss] {loss_str} [Metrics] {eval_metrics_str}\\n\"\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        collate_fn=start_end_collate,\n",
    "        batch_size=opt.bsz,\n",
    "        num_workers=opt.num_workers,\n",
    "        shuffle=True,\n",
    "        pin_memory=opt.pin_memory,\n",
    "    )\n",
    "\n",
    "    prev_best_score = 0.0\n",
    "    es_cnt = 0\n",
    "    # start_epoch = 0\n",
    "    if opt.start_epoch is None:\n",
    "        start_epoch = -1 if opt.eval_untrained else 0\n",
    "    else:\n",
    "        start_epoch = opt.start_epoch\n",
    "    save_submission_filename = \"latest_{}_{}_preds.jsonl\".format(\n",
    "        opt.dset_name, opt.eval_split_name\n",
    "    )\n",
    "    for epoch_i in trange(start_epoch, opt.n_epoch, desc=\"Epoch\"):\n",
    "        if epoch_i > -1:\n",
    "            train_epoch(\n",
    "                model, criterion, train_loader, optimizer, opt, epoch_i, tb_writer\n",
    "            )\n",
    "            lr_scheduler.step()\n",
    "        eval_epoch_interval = opt.eval_epoch\n",
    "        if opt.eval_path is not None and (epoch_i + 1) % eval_epoch_interval == 0:\n",
    "            with torch.no_grad():\n",
    "                metrics_no_nms, metrics_nms, eval_loss_meters, latest_file_paths = (\n",
    "                    eval_epoch(\n",
    "                        model,\n",
    "                        val_dataset,\n",
    "                        opt,\n",
    "                        save_submission_filename,\n",
    "                        epoch_i,\n",
    "                        criterion,\n",
    "                        tb_writer,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # log\n",
    "            to_write = opt.eval_log_txt_formatter.format(\n",
    "                time_str=time.strftime(\"%Y_%m_%d_%H_%M_%S\"),\n",
    "                epoch=epoch_i,\n",
    "                loss_str=\" \".join(\n",
    "                    [\"{} {:.4f}\".format(k, v.avg) for k, v in eval_loss_meters.items()]\n",
    "                ),\n",
    "                eval_metrics_str=json.dumps(metrics_no_nms),\n",
    "            )\n",
    "\n",
    "            with open(opt.eval_log_filepath, \"a\") as f:\n",
    "                f.write(to_write)\n",
    "            logger.info(\n",
    "                \"metrics_no_nms {}\".format(\n",
    "                    pprint.pformat(metrics_no_nms[\"brief\"], indent=4)\n",
    "                )\n",
    "            )\n",
    "            if metrics_nms is not None:\n",
    "                logger.info(\n",
    "                    \"metrics_nms {}\".format(\n",
    "                        pprint.pformat(metrics_nms[\"brief\"], indent=4)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            metrics = metrics_no_nms\n",
    "            for k, v in metrics[\"brief\"].items():\n",
    "                tb_writer.add_scalar(f\"Eval/{k}\", float(v), epoch_i + 1)\n",
    "\n",
    "            if opt.dset_name in [\"hl\"]:\n",
    "                stop_score = metrics[\"brief\"][\"MR-full-mAP\"]\n",
    "            else:\n",
    "                stop_score = (\n",
    "                    metrics[\"brief\"][\"MR-full-R1@0.7\"]\n",
    "                    + metrics[\"brief\"][\"MR-full-R1@0.5\"]\n",
    "                ) / 2\n",
    "\n",
    "            if stop_score > prev_best_score:\n",
    "                es_cnt = 0\n",
    "                prev_best_score = stop_score\n",
    "\n",
    "                checkpoint = {\n",
    "                    \"model\": model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "                    \"epoch\": epoch_i,\n",
    "                    \"opt\": opt,\n",
    "                }\n",
    "                torch.save(checkpoint, opt.ckpt_filepath.replace(\".ckpt\", \"_best.ckpt\"))\n",
    "\n",
    "                best_file_paths = [\n",
    "                    e.replace(\"latest\", \"best\") for e in latest_file_paths\n",
    "                ]\n",
    "                for src, tgt in zip(latest_file_paths, best_file_paths):\n",
    "                    os.renames(src, tgt)\n",
    "                logger.info(\"The checkpoint file has been updated.\")\n",
    "            else:\n",
    "                es_cnt += 1\n",
    "                if opt.max_es_cnt != -1 and es_cnt > opt.max_es_cnt:  # early stop\n",
    "                    with open(opt.train_log_filepath, \"a\") as f:\n",
    "                        f.write(f\"Early Stop at epoch {epoch_i}\")\n",
    "                    logger.info(\n",
    "                        f\"\\n>>>>> Early stop at epoch {epoch_i}  {prev_best_score}\\n\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "            # save ckpt\n",
    "            checkpoint = {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "                \"epoch\": epoch_i,\n",
    "                \"opt\": opt,\n",
    "            }\n",
    "            torch.save(checkpoint, opt.ckpt_filepath.replace(\".ckpt\", \"_latest.ckpt\"))\n",
    "\n",
    "        # save_interval = 10 if \"subs_train\" in opt.train_path else 50  # smaller for pretrain\n",
    "        # if (epoch_i + 1) % save_interval == 0 or (epoch_i + 1) % opt.lr_drop == 0:  # additional copies\n",
    "        #     checkpoint = {\n",
    "        #         \"model\": model.state_dict(),\n",
    "        #         \"optimizer\": optimizer.state_dict(),\n",
    "        #         \"epoch\": epoch_i,\n",
    "        #         \"opt\": opt\n",
    "        #     }\n",
    "        #     torch.save(checkpoint, opt.ckpt_filepath.replace(\".ckpt\", f\"_e{epoch_i:04d}.ckpt\"))\n",
    "\n",
    "        if opt.debug:\n",
    "            break\n",
    "\n",
    "    tb_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hl(\n",
    "    model, criterion, optimizer, lr_scheduler, train_dataset, val_dataset, opt\n",
    "):\n",
    "    if opt.device.type == \"cuda\":\n",
    "        logger.info(\"CUDA enabled.\")\n",
    "        model.to(opt.device)\n",
    "\n",
    "    tb_writer = SummaryWriter(opt.tensorboard_log_dir)\n",
    "    tb_writer.add_text(\"hyperparameters\", dict_to_markdown(vars(opt), max_str_len=None))\n",
    "    opt.train_log_txt_formatter = \"{time_str} [Epoch] {epoch:03d} [Loss] {loss_str}\\n\"\n",
    "    opt.eval_log_txt_formatter = \"{time_str} [Epoch] {epoch:03d} [Loss] {loss_str} [Metrics] {eval_metrics_str}\\n\"\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        collate_fn=start_end_collate,\n",
    "        batch_size=opt.bsz,\n",
    "        num_workers=opt.num_workers,\n",
    "        shuffle=True,\n",
    "        pin_memory=opt.pin_memory,\n",
    "    )\n",
    "\n",
    "    prev_best_score = 0.0\n",
    "    es_cnt = 0\n",
    "    # start_epoch = 0\n",
    "    if opt.start_epoch is None:\n",
    "        start_epoch = -1 if opt.eval_untrained else 0\n",
    "    else:\n",
    "        start_epoch = opt.start_epoch\n",
    "    save_submission_filename = \"latest_{}_{}_preds.jsonl\".format(\n",
    "        opt.dset_name, opt.eval_split_name\n",
    "    )\n",
    "    for epoch_i in trange(start_epoch, opt.n_epoch, desc=\"Epoch\"):\n",
    "        if epoch_i > -1:\n",
    "            train_epoch(\n",
    "                model, criterion, train_loader, optimizer, opt, epoch_i, tb_writer\n",
    "            )\n",
    "            lr_scheduler.step()\n",
    "        eval_epoch_interval = 5\n",
    "        if opt.eval_path is not None and (epoch_i + 1) % eval_epoch_interval == 0:\n",
    "            with torch.no_grad():\n",
    "                metrics_no_nms, metrics_nms, eval_loss_meters, latest_file_paths = (\n",
    "                    eval_epoch(\n",
    "                        model,\n",
    "                        val_dataset,\n",
    "                        opt,\n",
    "                        save_submission_filename,\n",
    "                        epoch_i,\n",
    "                        criterion,\n",
    "                        tb_writer,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # log\n",
    "            to_write = opt.eval_log_txt_formatter.format(\n",
    "                time_str=time.strftime(\"%Y_%m_%d_%H_%M_%S\"),\n",
    "                epoch=epoch_i,\n",
    "                loss_str=\" \".join(\n",
    "                    [\"{} {:.4f}\".format(k, v.avg) for k, v in eval_loss_meters.items()]\n",
    "                ),\n",
    "                eval_metrics_str=json.dumps(metrics_no_nms),\n",
    "            )\n",
    "\n",
    "            with open(opt.eval_log_filepath, \"a\") as f:\n",
    "                f.write(to_write)\n",
    "            logger.info(\n",
    "                \"metrics_no_nms {}\".format(\n",
    "                    pprint.pformat(metrics_no_nms[\"brief\"], indent=4)\n",
    "                )\n",
    "            )\n",
    "            if metrics_nms is not None:\n",
    "                logger.info(\n",
    "                    \"metrics_nms {}\".format(\n",
    "                        pprint.pformat(metrics_nms[\"brief\"], indent=4)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            metrics = metrics_no_nms\n",
    "            for k, v in metrics[\"brief\"].items():\n",
    "                tb_writer.add_scalar(f\"Eval/{k}\", float(v), epoch_i + 1)\n",
    "\n",
    "            # stop_score = metrics[\"brief\"][\"MR-full-mAP\"]\n",
    "            stop_score = metrics[\"brief\"][\"mAP\"]\n",
    "            if stop_score > prev_best_score:\n",
    "                es_cnt = 0\n",
    "                prev_best_score = stop_score\n",
    "\n",
    "                checkpoint = {\n",
    "                    \"model\": model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "                    \"epoch\": epoch_i,\n",
    "                    \"opt\": opt,\n",
    "                }\n",
    "                torch.save(checkpoint, opt.ckpt_filepath.replace(\".ckpt\", \"_best.ckpt\"))\n",
    "\n",
    "                best_file_paths = [\n",
    "                    e.replace(\"latest\", \"best\") for e in latest_file_paths\n",
    "                ]\n",
    "                for src, tgt in zip(latest_file_paths, best_file_paths):\n",
    "                    os.renames(src, tgt)\n",
    "                logger.info(\"The checkpoint file has been updated.\")\n",
    "            else:\n",
    "                es_cnt += 1\n",
    "                if opt.max_es_cnt != -1 and es_cnt > opt.max_es_cnt:  # early stop\n",
    "                    with open(opt.train_log_filepath, \"a\") as f:\n",
    "                        f.write(f\"Early Stop at epoch {epoch_i}\")\n",
    "                    logger.info(\n",
    "                        f\"\\n>>>>> Early stop at epoch {epoch_i}  {prev_best_score}\\n\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "            # save ckpt\n",
    "            checkpoint = {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "                \"epoch\": epoch_i,\n",
    "                \"opt\": opt,\n",
    "            }\n",
    "            torch.save(checkpoint, opt.ckpt_filepath.replace(\".ckpt\", \"_latest.ckpt\"))\n",
    "\n",
    "        save_interval = (\n",
    "            10 if \"subs_train\" in opt.train_path else 50\n",
    "        )  # smaller for pretrain\n",
    "        if (epoch_i + 1) % save_interval == 0 or (\n",
    "            epoch_i + 1\n",
    "        ) % opt.lr_drop == 0:  # additional copies\n",
    "            checkpoint = {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch_i,\n",
    "                \"opt\": opt,\n",
    "            }\n",
    "            torch.save(\n",
    "                checkpoint, opt.ckpt_filepath.replace(\".ckpt\", f\"_e{epoch_i:04d}.ckpt\")\n",
    "            )\n",
    "\n",
    "        if opt.debug:\n",
    "            break\n",
    "\n",
    "    tb_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_training():\n",
    "    logger.info(\"Setup config, data and model...\")\n",
    "    opt = BaseOptions().parse()\n",
    "    set_seed(opt.seed)\n",
    "    if opt.debug:  # keep the model run deterministically\n",
    "        # 'cudnn.benchmark = True' enabled auto finding the best algorithm for a specific input/net config.\n",
    "        # Enable this only when input size is fixed.\n",
    "        cudnn.benchmark = False\n",
    "        cudnn.deterministic = True\n",
    "\n",
    "    dataset_config = dict(\n",
    "        dset_name=opt.dset_name,\n",
    "        data_path=opt.train_path,\n",
    "        v_feat_dirs=opt.v_feat_dirs,\n",
    "        q_feat_dir=opt.t_feat_dir,\n",
    "        q_feat_type=\"last_hidden_state\",\n",
    "        max_q_l=opt.max_q_l,\n",
    "        max_v_l=opt.max_v_l,\n",
    "        ctx_mode=opt.ctx_mode,\n",
    "        data_ratio=opt.data_ratio,\n",
    "        normalize_v=not opt.no_norm_vfeat,\n",
    "        normalize_t=not opt.no_norm_tfeat,\n",
    "        clip_len=opt.clip_length,\n",
    "        max_windows=opt.max_windows,\n",
    "        span_loss_type=opt.span_loss_type,\n",
    "        txt_drop_ratio=opt.txt_drop_ratio,\n",
    "        dset_domain=opt.dset_domain,\n",
    "    )\n",
    "    dataset_config[\"data_path\"] = opt.train_path\n",
    "    train_dataset = StartEndDataset(**dataset_config)\n",
    "\n",
    "    if opt.eval_path is not None:\n",
    "        dataset_config[\"data_path\"] = opt.eval_path\n",
    "        dataset_config[\"txt_drop_ratio\"] = 0\n",
    "        dataset_config[\"q_feat_dir\"] = opt.t_feat_dir.replace(\n",
    "            \"sub_features\", \"text_features\"\n",
    "        )  # for pretraining\n",
    "        # dataset_config[\"load_labels\"] = False  # uncomment to calculate eval loss\n",
    "\n",
    "        eval_dataset = StartEndDataset(**dataset_config)\n",
    "\n",
    "    else:\n",
    "        eval_dataset = None\n",
    "\n",
    "    model, criterion, optimizer, lr_scheduler = setup_model(opt)\n",
    "    logger.info(f\"Model {model}\")\n",
    "    count_parameters(model)\n",
    "    logger.info(\"Start Training...\")\n",
    "\n",
    "    # For tvsum dataset, use train_hl function\n",
    "    if opt.dset_name in [\"tvsum\", \"youtube_uni\"]:\n",
    "        train_hl(\n",
    "            model, criterion, optimizer, lr_scheduler, train_dataset, eval_dataset, opt\n",
    "        )\n",
    "    else:\n",
    "        train(\n",
    "            model, criterion, optimizer, lr_scheduler, train_dataset, eval_dataset, opt\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        opt.ckpt_filepath.replace(\".ckpt\", \"_best.ckpt\"),\n",
    "        opt.eval_split_name,\n",
    "        opt.eval_path,\n",
    "        opt.debug,\n",
    "        opt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 09:56:57.393:INFO:__main__ - Setup config, data and model...\n",
      "usage: ipykernel_launcher.py [-h]\n",
      "                             [--dset_name {hl,tvsum,charadesSTA,tacos,nlq,youtube_uni}]\n",
      "                             [--dset_domain DSET_DOMAIN]\n",
      "                             [--eval_split_name EVAL_SPLIT_NAME] [--debug]\n",
      "                             [--data_ratio DATA_RATIO]\n",
      "                             [--results_root RESULTS_ROOT] [--exp_id EXP_ID]\n",
      "                             [--seed SEED] [--device DEVICE]\n",
      "                             [--num_workers NUM_WORKERS] [--no_pin_memory]\n",
      "                             [--lr LR] [--lr_drop LR_DROP] [--wd WD]\n",
      "                             [--n_epoch N_EPOCH] [--max_es_cnt MAX_ES_CNT]\n",
      "                             [--bsz BSZ] [--eval_bsz EVAL_BSZ]\n",
      "                             [--eval_epoch EVAL_EPOCH] [--grad_clip GRAD_CLIP]\n",
      "                             [--eval_untrained] [--resume RESUME]\n",
      "                             [--resume_all] [--start_epoch START_EPOCH]\n",
      "                             [--max_q_l MAX_Q_L] [--max_v_l MAX_V_L]\n",
      "                             [--clip_length CLIP_LENGTH]\n",
      "                             [--max_windows MAX_WINDOWS]\n",
      "                             [--train_path TRAIN_PATH] [--eval_path EVAL_PATH]\n",
      "                             [--no_norm_vfeat] [--no_norm_tfeat]\n",
      "                             [--v_feat_dirs V_FEAT_DIRS [V_FEAT_DIRS ...]]\n",
      "                             [--t_feat_dir T_FEAT_DIR]\n",
      "                             [--a_feat_dir A_FEAT_DIR]\n",
      "                             [--v_feat_dim V_FEAT_DIM]\n",
      "                             [--t_feat_dim T_FEAT_DIM]\n",
      "                             [--a_feat_dim A_FEAT_DIM] [--ctx_mode CTX_MODE]\n",
      "                             [--position_embedding {sine,learned}]\n",
      "                             [--enc_layers ENC_LAYERS]\n",
      "                             [--dec_layers DEC_LAYERS]\n",
      "                             [--t2v_layers T2V_LAYERS]\n",
      "                             [--sent_layers SENT_LAYERS]\n",
      "                             [--moment_layers MOMENT_LAYERS]\n",
      "                             [--dummy_layers DUMMY_LAYERS]\n",
      "                             [--dim_feedforward DIM_FEEDFORWARD]\n",
      "                             [--hidden_dim HIDDEN_DIM]\n",
      "                             [--input_dropout INPUT_DROPOUT]\n",
      "                             [--dropout DROPOUT]\n",
      "                             [--txt_drop_ratio TXT_DROP_RATIO] [--use_txt_pos]\n",
      "                             [--nheads NHEADS] [--num_queries NUM_QUERIES]\n",
      "                             [--num_dummies NUM_DUMMIES]\n",
      "                             [--total_prompts TOTAL_PROMPTS]\n",
      "                             [--num_prompts NUM_PROMPTS] [--pre_norm]\n",
      "                             [--n_input_proj N_INPUT_PROJ]\n",
      "                             [--contrastive_hdim CONTRASTIVE_HDIM]\n",
      "                             [--temperature TEMPERATURE]\n",
      "                             [--saliency_margin SALIENCY_MARGIN]\n",
      "                             [--no_aux_loss] [--span_loss_type {l1,ce}]\n",
      "                             [--contrastive_align_loss]\n",
      "                             [--set_cost_span SET_COST_SPAN]\n",
      "                             [--set_cost_giou SET_COST_GIOU]\n",
      "                             [--set_cost_class SET_COST_CLASS]\n",
      "                             [--lw_saliency LW_SALIENCY] [--lw_wattn LW_WATTN]\n",
      "                             [--lw_ms_align LW_MS_ALIGN]\n",
      "                             [--lw_distill LW_DISTILL]\n",
      "                             [--span_loss_coef SPAN_LOSS_COEF]\n",
      "                             [--giou_loss_coef GIOU_LOSS_COEF]\n",
      "                             [--label_loss_coef LABEL_LOSS_COEF]\n",
      "                             [--eos_coef EOS_COEF]\n",
      "                             [--contrastive_align_loss_coef CONTRASTIVE_ALIGN_LOSS_COEF]\n",
      "                             [--no_sort_results]\n",
      "                             [--max_before_nms MAX_BEFORE_NMS]\n",
      "                             [--max_after_nms MAX_AFTER_NMS]\n",
      "                             [--conf_thd CONF_THD] [--nms_thd NMS_THD]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=\"c:\\Users\\Ashiq and Adnan\\AppData\\Roaming\\jupyter\\runtime\\kernel-v3244e6ba853c6a156d1624dae3ff183f0ac6a262c.json\"\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\CG-Detr\\CGDETR\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    best_ckpt_path, eval_split_name, eval_path, debug = start_training()\n",
    "    if not debug:\n",
    "        input_args = [\"--resume\", best_ckpt_path,\n",
    "                      \"--eval_split_name\", eval_split_name,\n",
    "                      \"--eval_path\", eval_path]\n",
    "\n",
    "        import sys\n",
    "        sys.argv[1:] = input_args\n",
    "        logger.info(\"\\n\\n\\nFINISHED TRAINING!!!\")\n",
    "        logger.info(\"Evaluating model at {}\".format(best_ckpt_path))\n",
    "        logger.info(\"Input args {}\".format(sys.argv[1:]))\n",
    "        start_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_on_video.data_utils import ClipFeatureExtractor\n",
    "from run_on_video.model_utils import build_inference_model\n",
    "from utils.tensor_utils import pad_sequences_1d\n",
    "from cg_detr.span_utils import span_cxw_to_xx\n",
    "from utils.basic_utils import l2_normalize_np_array\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pytube import YouTube\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        return [json.loads(l.strip(\"\\n\")) for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGDETRPredictor:\n",
    "    def __init__(self, ckpt_path, clip_model_name_or_path=\"ViT-B/32\", device=\"cuda\"):\n",
    "        self.clip_len = 2  # seconds\n",
    "        self.device = device\n",
    "        print(\"Loading feature extractors...\")\n",
    "        self.feature_extractor = ClipFeatureExtractor(\n",
    "            framerate=1/self.clip_len, size=224, centercrop=True,\n",
    "            model_name_or_path=clip_model_name_or_path, device=device\n",
    "        )\n",
    "        print(\"Loading trained CG-DETR model...\")\n",
    "        self.model = build_inference_model(ckpt_path).to(self.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def localize_moment(self, video_path, query_list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            video_path: str, path to the video file\n",
    "            query_list: List[str], each str is a query for this video\n",
    "        \"\"\"\n",
    "        # construct model inputs\n",
    "        n_query = len(query_list)\n",
    "        video_feats = self.feature_extractor.encode_video(video_path)\n",
    "        video_feats = F.normalize(video_feats, dim=-1, eps=1e-5)\n",
    "        n_frames = len(video_feats)\n",
    "        # add tef\n",
    "        tef_st = torch.arange(0, n_frames, 1.0) / n_frames\n",
    "        tef_ed = tef_st + 1.0 / n_frames\n",
    "        tef = torch.stack([tef_st, tef_ed], dim=1).to(self.device)  # (n_frames, 2)\n",
    "        video_feats = torch.cat([video_feats, tef], dim=1)\n",
    "        assert n_frames <= 75, \"The positional embedding of this pretrained CGDETR only support video up \" \\\n",
    "                               \"to 150 secs (i.e., 75 2-sec clips) in length\"\n",
    "        video_feats = video_feats.unsqueeze(0).repeat(n_query, 1, 1)  # (#text, T, d)\n",
    "        video_mask = torch.ones(n_query, n_frames).to(self.device)\n",
    "        query_feats = self.feature_extractor.encode_text(query_list)  # #text * (L, d)\n",
    "        query_feats, query_mask = pad_sequences_1d(\n",
    "            query_feats, dtype=torch.float32, device=self.device, fixed_length=None)\n",
    "        query_feats = F.normalize(query_feats, dim=-1, eps=1e-5)\n",
    "        model_inputs = dict(\n",
    "            src_vid=video_feats,\n",
    "            src_vid_mask=video_mask,\n",
    "            src_txt=query_feats,\n",
    "            src_txt_mask=query_mask,\n",
    "            vid=None,\n",
    "            qid=None\n",
    "        )\n",
    "\n",
    "        # decode outputs\n",
    "        outputs = self.model(**model_inputs)\n",
    "        # #moment_queries refers to the positional embeddings in CGDETR's decoder, not the input text query\n",
    "        prob = F.softmax(outputs[\"pred_logits\"], -1)  # (batch_size, #moment_queries=10, #classes=2)\n",
    "        scores = prob[..., 0]  # * (batch_size, #moment_queries)  foreground label is 0, we directly take it\n",
    "        pred_spans = outputs[\"pred_spans\"]  # (bsz, #moment_queries, 2)\n",
    "        _saliency_scores = outputs[\"saliency_scores\"].half()  # (bsz, L)\n",
    "        saliency_scores = []\n",
    "        valid_vid_lengths = model_inputs[\"src_vid_mask\"].sum(1).cpu().tolist()\n",
    "        for j in range(len(valid_vid_lengths)):\n",
    "            _score = _saliency_scores[j, :int(valid_vid_lengths[j])].tolist()\n",
    "            _score = [round(e, 4) for e in _score]\n",
    "            saliency_scores.append(_score)\n",
    "\n",
    "        # compose predictions\n",
    "        predictions = []\n",
    "        video_duration = n_frames * self.clip_len\n",
    "        for idx, (spans, score) in enumerate(zip(pred_spans.cpu(), scores.cpu())):\n",
    "            spans = span_cxw_to_xx(spans) * video_duration\n",
    "            # # (#queries, 3), [st(float), ed(float), score(float)]\n",
    "            cur_ranked_preds = torch.cat([spans, score[:, None]], dim=1).tolist()\n",
    "            cur_ranked_preds = sorted(cur_ranked_preds, key=lambda x: x[2], reverse=True)\n",
    "            cur_ranked_preds = [[float(f\"{e:.4f}\") for e in row] for row in cur_ranked_preds]\n",
    "            cur_query_pred = dict(\n",
    "                query=query_list[idx],  # str\n",
    "                vid=video_path,\n",
    "                pred_relevant_windows=cur_ranked_preds,  # List([st(float), ed(float), score(float)])\n",
    "                pred_saliency_scores=saliency_scores[idx]  # List(float), len==n_frames, scores for each frame\n",
    "            )\n",
    "            predictions.append(cur_query_pred)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example():\n",
    "\n",
    "    '''\n",
    "    1) If you want to use the custom data, leave the url empty\n",
    "    '''\n",
    "    youtube_url = ''\n",
    "    vid_st_sec, vid_ed_sec = 0.0, 0.0\n",
    "    desired_query = ''\n",
    "\n",
    "    '''\n",
    "    2) If you want to run with a video from youtube, please enter the youtube_url, [st, ed] in seconds, and custom query\n",
    "    # Maximum duration is 150 secs or lower. Recommend to use less than 150 secs.\n",
    "    youtube_url = 'https://www.youtube.com/watch?v=geklhsKfw7I'\n",
    "    vid_st_sec, vid_ed_sec = 60.0, 205.0\n",
    "    desired_query = 'Girls having fun out side shop'\n",
    "    '''\n",
    "    # youtube_url = 'https://www.youtube.com/watch?v=geklhsKfw7I'\n",
    "    # vid_st_sec, vid_ed_sec = 60.0, 205.0\n",
    "    # desired_query = 'Girls having fun out side shop'\n",
    "\n",
    "\n",
    "    # load example data\n",
    "    from utils.basic_utils import load_jsonl\n",
    "\n",
    "    if youtube_url != '':\n",
    "        # vid = info['vid'] # \"vid\": \"NUsG9BgSes0_210.0_360.0\"\n",
    "        queries = []\n",
    "        queries.append({})\n",
    "        file_name = youtube_url.split('/')[-1][8:] + '_' + str(vid_st_sec) + '_' + str(vid_ed_sec) + '.mp4'\n",
    "        if os.path.exists(os.path.join('run_on_video/example', file_name)):\n",
    "            video_path = os.path.join('run_on_video/example', file_name)\n",
    "            queries[0]['query'] = desired_query\n",
    "        else:\n",
    "            try:\n",
    "                yt = YouTube(youtube_url)\n",
    "                stream = yt.streams.get_highest_resolution()\n",
    "                video_path = os.path.join('./run_on_video/example', file_name)\n",
    "                stream.download(output_path='./run_on_video/example', filename=file_name)\n",
    "            except:\n",
    "                print('Error downloading video')\n",
    "                exit(1)\n",
    "\n",
    "\n",
    "\n",
    "        with VideoFileClip(video_path) as video:\n",
    "            new = video.subclip(vid_st_sec, vid_ed_sec)\n",
    "            new.write_videofile(video_path, audio_codec='aac')\n",
    "\n",
    "\n",
    "        queries[0]['query'] = 'A woman is talking to a camera.'\n",
    "    else:\n",
    "        video_path = \"run_on_video/example/RoripwjYFp8_60.0_210.0.mp4\"\n",
    "        query_path = \"run_on_video/example/queries.jsonl\"\n",
    "        queries = load_jsonl(query_path)\n",
    "    query_text_list = [e[\"query\"] for e in queries]\n",
    "    ckpt_path = \"run_on_video/CLIP_ckpt/qvhighlights_onlyCLIP/model_best.ckpt\"\n",
    "\n",
    "    # run predictions\n",
    "    print(\"Build models...\")\n",
    "    clip_model_name_or_path = \"ViT-B/32\"\n",
    "    # clip_model_name_or_path = \"tmp/ViT-B-32.pt\"\n",
    "    cg_detr_predictor = CGDETRPredictor(\n",
    "        ckpt_path=ckpt_path,\n",
    "        clip_model_name_or_path=clip_model_name_or_path,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "    print(\"Run prediction...\")\n",
    "    predictions = cg_detr_predictor.localize_moment(\n",
    "        video_path=video_path, query_list=query_text_list)\n",
    "\n",
    "    # print data\n",
    "    for idx, query_data in enumerate(queries):\n",
    "        print(\"-\"*30 + f\"idx{idx}\")\n",
    "        print(f\">> query: {query_data['query']}\")\n",
    "        print(f\">> video_path: {video_path}\")\n",
    "        print(f\">> Predicted moments ([start_in_seconds, end_in_seconds, score]): \"\n",
    "              f\"{predictions[idx]['pred_relevant_windows']}\")\n",
    "        pred_saliency_scores = torch.Tensor(predictions[idx]['pred_saliency_scores'])\n",
    "        bias = 0 - pred_saliency_scores.min()\n",
    "        pred_saliency_scores += bias\n",
    "        print(f\">> Most saliency clip is (for all 2-sec clip): \"\n",
    "              f\"{pred_saliency_scores.argmax()}\")\n",
    "        print(f\">> Predicted saliency scores (for all 2-sec clip): \"\n",
    "              f\"{pred_saliency_scores.tolist()}\")\n",
    "        if youtube_url == '':\n",
    "            print(f\">> GT moments: {query_data['relevant_windows']}\")\n",
    "            print(f\">> GT saliency scores (only localized 2-sec clips): {query_data['saliency_scores']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build models...\n",
      "Loading feature extractors...\n",
      "Loading CLIP models\n",
      "Loading trained CG-DETR model...\n",
      "Run prediction...\n",
      "ffprobe failed at: run_on_video/example/RoripwjYFp8_60.0_210.0.mp4\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'dict' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20976\\1516782024.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mrun_example\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20976\\1037851205.py\u001b[0m in \u001b[0;36mrun_example\u001b[1;34m()\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Run prediction...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     predictions = cg_detr_predictor.localize_moment(\n\u001b[1;32m---> 69\u001b[1;33m         video_path=video_path, query_list=query_text_list)\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;31m# print data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\CG-Detr\\CGDETR\\venv\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20976\\2661380818.py\u001b[0m in \u001b[0;36mlocalize_moment\u001b[1;34m(self, video_path, query_list)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# construct model inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mn_query\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mvideo_feats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode_video\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mvideo_feats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_feats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mn_frames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_feats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\CG-Detr\\CGDETR\\venv\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\CG-Detr\\CGDETR\\run_on_video\\data_utils.py\u001b[0m in \u001b[0;36mencode_video\u001b[1;34m(self, video_path, bsz)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode_video\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo_path\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mvideo_frames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideo_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_video_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (T, H, W, 3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mvideo_frames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideo_preprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_frames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mn_frames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_frames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mn_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_frames\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbsz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\CG-Detr\\CGDETR\\run_on_video\\data_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'dict' and 'float'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
