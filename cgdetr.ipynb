{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pprint\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from cg_detr.config import BaseOptions\n",
    "from cg_detr.start_end_dataset import \\\n",
    "    StartEndDataset, start_end_collate, prepare_batch_inputs\n",
    "from cg_detr.inference import eval_epoch, start_inference, setup_model\n",
    "from utils.basic_utils import AverageMeter, dict_to_markdown\n",
    "from utils.model_utils import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s.%(msecs)03d:%(levelname)s:%(name)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, use_cuda=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, use_cuda=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def train_epoch(model, criterion, train_loader, optimizer, opt, epoch_i, tb_writer):\n",
    "    logger.info(f\"[Epoch {epoch_i+1}]\")\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "\n",
    "    # init meters\n",
    "    time_meters = defaultdict(AverageMeter)\n",
    "    loss_meters = defaultdict(AverageMeter)\n",
    "\n",
    "    num_training_examples = len(train_loader)\n",
    "    timer_dataloading = time.time()\n",
    "    for batch_idx, batch in tqdm(\n",
    "        enumerate(train_loader), desc=\"Training Iteration\", total=num_training_examples\n",
    "    ):\n",
    "        time_meters[\"dataloading_time\"].update(time.time() - timer_dataloading)\n",
    "\n",
    "        timer_start = time.time()\n",
    "\n",
    "        model_inputs, targets = prepare_batch_inputs(\n",
    "            batch[1], opt.device, non_blocking=opt.pin_memory\n",
    "        )\n",
    "\n",
    "        time_meters[\"prepare_inputs_time\"].update(time.time() - timer_start)\n",
    "        timer_start = time.time()\n",
    "\n",
    "        outputs = model(**model_inputs, targets=targets)\n",
    "        loss_dict = criterion(outputs, targets)\n",
    "        weight_dict = criterion.weight_dict\n",
    "        losses = sum(\n",
    "            loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict\n",
    "        )\n",
    "        time_meters[\"model_forward_time\"].update(time.time() - timer_start)\n",
    "\n",
    "        timer_start = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        if opt.grad_clip > 0:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip)\n",
    "        optimizer.step()\n",
    "        time_meters[\"model_backward_time\"].update(time.time() - timer_start)\n",
    "\n",
    "        loss_dict[\"loss_overall\"] = float(losses)  # for logging only\n",
    "        for k, v in loss_dict.items():\n",
    "            loss_meters[k].update(\n",
    "                float(v) * weight_dict[k] if k in weight_dict else float(v)\n",
    "            )\n",
    "\n",
    "        timer_dataloading = time.time()\n",
    "        if opt.debug and batch_idx == 3:\n",
    "            break\n",
    "\n",
    "    # print/add logs\n",
    "    tb_writer.add_scalar(\n",
    "        \"Train/lr\", float(optimizer.param_groups[0][\"lr\"]), epoch_i + 1\n",
    "    )\n",
    "    for k, v in loss_meters.items():\n",
    "        tb_writer.add_scalar(\"Train/{}\".format(k), v.avg, epoch_i + 1)\n",
    "\n",
    "    to_write = opt.train_log_txt_formatter.format(\n",
    "        time_str=time.strftime(\"%Y_%m_%d_%H_%M_%S\"),\n",
    "        epoch=epoch_i + 1,\n",
    "        loss_str=\" \".join(\n",
    "            [\"{} {:.4f}\".format(k, v.avg) for k, v in loss_meters.items()]\n",
    "        ),\n",
    "    )\n",
    "    with open(opt.train_log_filepath, \"a\") as f:\n",
    "        f.write(to_write)\n",
    "\n",
    "    logger.info(\"Epoch time stats:\")\n",
    "    for name, meter in time_meters.items():\n",
    "        d = {k: f\"{getattr(meter, k):.4f}\" for k in [\"max\", \"min\", \"avg\"]}\n",
    "        logger.info(f\"{name} ==> {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, lr_scheduler, train_dataset, val_dataset, opt):\n",
    "    if opt.device.type == \"cuda\":\n",
    "        logger.info(\"CUDA enabled.\")\n",
    "        model.to(opt.device)\n",
    "\n",
    "    tb_writer = SummaryWriter(opt.tensorboard_log_dir)\n",
    "    tb_writer.add_text(\"hyperparameters\", dict_to_markdown(vars(opt), max_str_len=None))\n",
    "    opt.train_log_txt_formatter = \"{time_str} [Epoch] {epoch:03d} [Loss] {loss_str}\\n\"\n",
    "    opt.eval_log_txt_formatter = \"{time_str} [Epoch] {epoch:03d} [Loss] {loss_str} [Metrics] {eval_metrics_str}\\n\"\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        collate_fn=start_end_collate,\n",
    "        batch_size=opt.bsz,\n",
    "        num_workers=opt.num_workers,\n",
    "        shuffle=True,\n",
    "        pin_memory=opt.pin_memory,\n",
    "    )\n",
    "\n",
    "    prev_best_score = 0.0\n",
    "    es_cnt = 0\n",
    "    # start_epoch = 0\n",
    "    if opt.start_epoch is None:\n",
    "        start_epoch = -1 if opt.eval_untrained else 0\n",
    "    else:\n",
    "        start_epoch = opt.start_epoch\n",
    "    save_submission_filename = \"latest_{}_{}_preds.jsonl\".format(\n",
    "        opt.dset_name, opt.eval_split_name\n",
    "    )\n",
    "    for epoch_i in trange(start_epoch, opt.n_epoch, desc=\"Epoch\"):\n",
    "        if epoch_i > -1:\n",
    "            train_epoch(\n",
    "                model, criterion, train_loader, optimizer, opt, epoch_i, tb_writer\n",
    "            )\n",
    "            lr_scheduler.step()\n",
    "        eval_epoch_interval = opt.eval_epoch\n",
    "        if opt.eval_path is not None and (epoch_i + 1) % eval_epoch_interval == 0:\n",
    "            with torch.no_grad():\n",
    "                metrics_no_nms, metrics_nms, eval_loss_meters, latest_file_paths = (\n",
    "                    eval_epoch(\n",
    "                        model,\n",
    "                        val_dataset,\n",
    "                        opt,\n",
    "                        save_submission_filename,\n",
    "                        epoch_i,\n",
    "                        criterion,\n",
    "                        tb_writer,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # log\n",
    "            to_write = opt.eval_log_txt_formatter.format(\n",
    "                time_str=time.strftime(\"%Y_%m_%d_%H_%M_%S\"),\n",
    "                epoch=epoch_i,\n",
    "                loss_str=\" \".join(\n",
    "                    [\"{} {:.4f}\".format(k, v.avg) for k, v in eval_loss_meters.items()]\n",
    "                ),\n",
    "                eval_metrics_str=json.dumps(metrics_no_nms),\n",
    "            )\n",
    "\n",
    "            with open(opt.eval_log_filepath, \"a\") as f:\n",
    "                f.write(to_write)\n",
    "            logger.info(\n",
    "                \"metrics_no_nms {}\".format(\n",
    "                    pprint.pformat(metrics_no_nms[\"brief\"], indent=4)\n",
    "                )\n",
    "            )\n",
    "            if metrics_nms is not None:\n",
    "                logger.info(\n",
    "                    \"metrics_nms {}\".format(\n",
    "                        pprint.pformat(metrics_nms[\"brief\"], indent=4)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            metrics = metrics_no_nms\n",
    "            for k, v in metrics[\"brief\"].items():\n",
    "                tb_writer.add_scalar(f\"Eval/{k}\", float(v), epoch_i + 1)\n",
    "\n",
    "            if opt.dset_name in [\"hl\"]:\n",
    "                stop_score = metrics[\"brief\"][\"MR-full-mAP\"]\n",
    "            else:\n",
    "                stop_score = (\n",
    "                    metrics[\"brief\"][\"MR-full-R1@0.7\"]\n",
    "                    + metrics[\"brief\"][\"MR-full-R1@0.5\"]\n",
    "                ) / 2\n",
    "\n",
    "            if stop_score > prev_best_score:\n",
    "                es_cnt = 0\n",
    "                prev_best_score = stop_score\n",
    "\n",
    "                checkpoint = {\n",
    "                    \"model\": model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "                    \"epoch\": epoch_i,\n",
    "                    \"opt\": opt,\n",
    "                }\n",
    "                torch.save(checkpoint, opt.ckpt_filepath.replace(\".ckpt\", \"_best.ckpt\"))\n",
    "\n",
    "                best_file_paths = [\n",
    "                    e.replace(\"latest\", \"best\") for e in latest_file_paths\n",
    "                ]\n",
    "                for src, tgt in zip(latest_file_paths, best_file_paths):\n",
    "                    os.renames(src, tgt)\n",
    "                logger.info(\"The checkpoint file has been updated.\")\n",
    "            else:\n",
    "                es_cnt += 1\n",
    "                if opt.max_es_cnt != -1 and es_cnt > opt.max_es_cnt:  # early stop\n",
    "                    with open(opt.train_log_filepath, \"a\") as f:\n",
    "                        f.write(f\"Early Stop at epoch {epoch_i}\")\n",
    "                    logger.info(\n",
    "                        f\"\\n>>>>> Early stop at epoch {epoch_i}  {prev_best_score}\\n\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "            # save ckpt\n",
    "            checkpoint = {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "                \"epoch\": epoch_i,\n",
    "                \"opt\": opt,\n",
    "            }\n",
    "            torch.save(checkpoint, opt.ckpt_filepath.replace(\".ckpt\", \"_latest.ckpt\"))\n",
    "\n",
    "        # save_interval = 10 if \"subs_train\" in opt.train_path else 50  # smaller for pretrain\n",
    "        # if (epoch_i + 1) % save_interval == 0 or (epoch_i + 1) % opt.lr_drop == 0:  # additional copies\n",
    "        #     checkpoint = {\n",
    "        #         \"model\": model.state_dict(),\n",
    "        #         \"optimizer\": optimizer.state_dict(),\n",
    "        #         \"epoch\": epoch_i,\n",
    "        #         \"opt\": opt\n",
    "        #     }\n",
    "        #     torch.save(checkpoint, opt.ckpt_filepath.replace(\".ckpt\", f\"_e{epoch_i:04d}.ckpt\"))\n",
    "\n",
    "        if opt.debug:\n",
    "            break\n",
    "\n",
    "    tb_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hl(\n",
    "    model, criterion, optimizer, lr_scheduler, train_dataset, val_dataset, opt\n",
    "):\n",
    "    if opt.device.type == \"cuda\":\n",
    "        logger.info(\"CUDA enabled.\")\n",
    "        model.to(opt.device)\n",
    "\n",
    "    tb_writer = SummaryWriter(opt.tensorboard_log_dir)\n",
    "    tb_writer.add_text(\"hyperparameters\", dict_to_markdown(vars(opt), max_str_len=None))\n",
    "    opt.train_log_txt_formatter = \"{time_str} [Epoch] {epoch:03d} [Loss] {loss_str}\\n\"\n",
    "    opt.eval_log_txt_formatter = \"{time_str} [Epoch] {epoch:03d} [Loss] {loss_str} [Metrics] {eval_metrics_str}\\n\"\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        collate_fn=start_end_collate,\n",
    "        batch_size=opt.bsz,\n",
    "        num_workers=opt.num_workers,\n",
    "        shuffle=True,\n",
    "        pin_memory=opt.pin_memory,\n",
    "    )\n",
    "\n",
    "    prev_best_score = 0.0\n",
    "    es_cnt = 0\n",
    "    # start_epoch = 0\n",
    "    if opt.start_epoch is None:\n",
    "        start_epoch = -1 if opt.eval_untrained else 0\n",
    "    else:\n",
    "        start_epoch = opt.start_epoch\n",
    "    save_submission_filename = \"latest_{}_{}_preds.jsonl\".format(\n",
    "        opt.dset_name, opt.eval_split_name\n",
    "    )\n",
    "    for epoch_i in trange(start_epoch, opt.n_epoch, desc=\"Epoch\"):\n",
    "        if epoch_i > -1:\n",
    "            train_epoch(\n",
    "                model, criterion, train_loader, optimizer, opt, epoch_i, tb_writer\n",
    "            )\n",
    "            lr_scheduler.step()\n",
    "        eval_epoch_interval = 5\n",
    "        if opt.eval_path is not None and (epoch_i + 1) % eval_epoch_interval == 0:\n",
    "            with torch.no_grad():\n",
    "                metrics_no_nms, metrics_nms, eval_loss_meters, latest_file_paths = (\n",
    "                    eval_epoch(\n",
    "                        model,\n",
    "                        val_dataset,\n",
    "                        opt,\n",
    "                        save_submission_filename,\n",
    "                        epoch_i,\n",
    "                        criterion,\n",
    "                        tb_writer,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # log\n",
    "            to_write = opt.eval_log_txt_formatter.format(\n",
    "                time_str=time.strftime(\"%Y_%m_%d_%H_%M_%S\"),\n",
    "                epoch=epoch_i,\n",
    "                loss_str=\" \".join(\n",
    "                    [\"{} {:.4f}\".format(k, v.avg) for k, v in eval_loss_meters.items()]\n",
    "                ),\n",
    "                eval_metrics_str=json.dumps(metrics_no_nms),\n",
    "            )\n",
    "\n",
    "            with open(opt.eval_log_filepath, \"a\") as f:\n",
    "                f.write(to_write)\n",
    "            logger.info(\n",
    "                \"metrics_no_nms {}\".format(\n",
    "                    pprint.pformat(metrics_no_nms[\"brief\"], indent=4)\n",
    "                )\n",
    "            )\n",
    "            if metrics_nms is not None:\n",
    "                logger.info(\n",
    "                    \"metrics_nms {}\".format(\n",
    "                        pprint.pformat(metrics_nms[\"brief\"], indent=4)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            metrics = metrics_no_nms\n",
    "            for k, v in metrics[\"brief\"].items():\n",
    "                tb_writer.add_scalar(f\"Eval/{k}\", float(v), epoch_i + 1)\n",
    "\n",
    "            # stop_score = metrics[\"brief\"][\"MR-full-mAP\"]\n",
    "            stop_score = metrics[\"brief\"][\"mAP\"]\n",
    "            if stop_score > prev_best_score:\n",
    "                es_cnt = 0\n",
    "                prev_best_score = stop_score\n",
    "\n",
    "                checkpoint = {\n",
    "                    \"model\": model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "                    \"epoch\": epoch_i,\n",
    "                    \"opt\": opt,\n",
    "                }\n",
    "                torch.save(checkpoint, opt.ckpt_filepath.replace(\".ckpt\", \"_best.ckpt\"))\n",
    "\n",
    "                best_file_paths = [\n",
    "                    e.replace(\"latest\", \"best\") for e in latest_file_paths\n",
    "                ]\n",
    "                for src, tgt in zip(latest_file_paths, best_file_paths):\n",
    "                    os.renames(src, tgt)\n",
    "                logger.info(\"The checkpoint file has been updated.\")\n",
    "            else:\n",
    "                es_cnt += 1\n",
    "                if opt.max_es_cnt != -1 and es_cnt > opt.max_es_cnt:  # early stop\n",
    "                    with open(opt.train_log_filepath, \"a\") as f:\n",
    "                        f.write(f\"Early Stop at epoch {epoch_i}\")\n",
    "                    logger.info(\n",
    "                        f\"\\n>>>>> Early stop at epoch {epoch_i}  {prev_best_score}\\n\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "            # save ckpt\n",
    "            checkpoint = {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "                \"epoch\": epoch_i,\n",
    "                \"opt\": opt,\n",
    "            }\n",
    "            torch.save(checkpoint, opt.ckpt_filepath.replace(\".ckpt\", \"_latest.ckpt\"))\n",
    "\n",
    "        save_interval = (\n",
    "            10 if \"subs_train\" in opt.train_path else 50\n",
    "        )  # smaller for pretrain\n",
    "        if (epoch_i + 1) % save_interval == 0 or (\n",
    "            epoch_i + 1\n",
    "        ) % opt.lr_drop == 0:  # additional copies\n",
    "            checkpoint = {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch_i,\n",
    "                \"opt\": opt,\n",
    "            }\n",
    "            torch.save(\n",
    "                checkpoint, opt.ckpt_filepath.replace(\".ckpt\", f\"_e{epoch_i:04d}.ckpt\")\n",
    "            )\n",
    "\n",
    "        if opt.debug:\n",
    "            break\n",
    "\n",
    "    tb_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_training():\n",
    "    logger.info(\"Setup config, data and model...\")\n",
    "    opt = BaseOptions().parse()\n",
    "    set_seed(opt.seed)\n",
    "    if opt.debug:  # keep the model run deterministically\n",
    "        # 'cudnn.benchmark = True' enabled auto finding the best algorithm for a specific input/net config.\n",
    "        # Enable this only when input size is fixed.\n",
    "        cudnn.benchmark = False\n",
    "        cudnn.deterministic = True\n",
    "\n",
    "    dataset_config = dict(\n",
    "        dset_name=opt.dset_name,\n",
    "        data_path=opt.train_path,\n",
    "        v_feat_dirs=opt.v_feat_dirs,\n",
    "        q_feat_dir=opt.t_feat_dir,\n",
    "        q_feat_type=\"last_hidden_state\",\n",
    "        max_q_l=opt.max_q_l,\n",
    "        max_v_l=opt.max_v_l,\n",
    "        ctx_mode=opt.ctx_mode,\n",
    "        data_ratio=opt.data_ratio,\n",
    "        normalize_v=not opt.no_norm_vfeat,\n",
    "        normalize_t=not opt.no_norm_tfeat,\n",
    "        clip_len=opt.clip_length,\n",
    "        max_windows=opt.max_windows,\n",
    "        span_loss_type=opt.span_loss_type,\n",
    "        txt_drop_ratio=opt.txt_drop_ratio,\n",
    "        dset_domain=opt.dset_domain,\n",
    "    )\n",
    "    dataset_config[\"data_path\"] = opt.train_path\n",
    "    train_dataset = StartEndDataset(**dataset_config)\n",
    "\n",
    "    if opt.eval_path is not None:\n",
    "        dataset_config[\"data_path\"] = opt.eval_path\n",
    "        dataset_config[\"txt_drop_ratio\"] = 0\n",
    "        dataset_config[\"q_feat_dir\"] = opt.t_feat_dir.replace(\n",
    "            \"sub_features\", \"text_features\"\n",
    "        )  # for pretraining\n",
    "        # dataset_config[\"load_labels\"] = False  # uncomment to calculate eval loss\n",
    "\n",
    "        eval_dataset = StartEndDataset(**dataset_config)\n",
    "\n",
    "    else:\n",
    "        eval_dataset = None\n",
    "\n",
    "    model, criterion, optimizer, lr_scheduler = setup_model(opt)\n",
    "    logger.info(f\"Model {model}\")\n",
    "    count_parameters(model)\n",
    "    logger.info(\"Start Training...\")\n",
    "\n",
    "    # For tvsum dataset, use train_hl function\n",
    "    if opt.dset_name in [\"tvsum\", \"youtube_uni\"]:\n",
    "        train_hl(\n",
    "            model, criterion, optimizer, lr_scheduler, train_dataset, eval_dataset, opt\n",
    "        )\n",
    "    else:\n",
    "        train(\n",
    "            model, criterion, optimizer, lr_scheduler, train_dataset, eval_dataset, opt\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        opt.ckpt_filepath.replace(\".ckpt\", \"_best.ckpt\"),\n",
    "        opt.eval_split_name,\n",
    "        opt.eval_path,\n",
    "        opt.debug,\n",
    "        opt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 20:36:30.488:INFO:__main__ - Setup config, data and model...\n",
      "usage: ipykernel_launcher.py [-h]\n",
      "                             [--dset_name {hl,tvsum,charadesSTA,tacos,nlq,youtube_uni}]\n",
      "                             [--dset_domain DSET_DOMAIN]\n",
      "                             [--eval_split_name EVAL_SPLIT_NAME] [--debug]\n",
      "                             [--data_ratio DATA_RATIO]\n",
      "                             [--results_root RESULTS_ROOT] [--exp_id EXP_ID]\n",
      "                             [--seed SEED] [--device DEVICE]\n",
      "                             [--num_workers NUM_WORKERS] [--no_pin_memory]\n",
      "                             [--lr LR] [--lr_drop LR_DROP] [--wd WD]\n",
      "                             [--n_epoch N_EPOCH] [--max_es_cnt MAX_ES_CNT]\n",
      "                             [--bsz BSZ] [--eval_bsz EVAL_BSZ]\n",
      "                             [--eval_epoch EVAL_EPOCH] [--grad_clip GRAD_CLIP]\n",
      "                             [--eval_untrained] [--resume RESUME]\n",
      "                             [--resume_all] [--start_epoch START_EPOCH]\n",
      "                             [--max_q_l MAX_Q_L] [--max_v_l MAX_V_L]\n",
      "                             [--clip_length CLIP_LENGTH]\n",
      "                             [--max_windows MAX_WINDOWS]\n",
      "                             [--train_path TRAIN_PATH] [--eval_path EVAL_PATH]\n",
      "                             [--no_norm_vfeat] [--no_norm_tfeat]\n",
      "                             [--v_feat_dirs V_FEAT_DIRS [V_FEAT_DIRS ...]]\n",
      "                             [--t_feat_dir T_FEAT_DIR]\n",
      "                             [--a_feat_dir A_FEAT_DIR]\n",
      "                             [--v_feat_dim V_FEAT_DIM]\n",
      "                             [--t_feat_dim T_FEAT_DIM]\n",
      "                             [--a_feat_dim A_FEAT_DIM] [--ctx_mode CTX_MODE]\n",
      "                             [--position_embedding {sine,learned}]\n",
      "                             [--enc_layers ENC_LAYERS]\n",
      "                             [--dec_layers DEC_LAYERS]\n",
      "                             [--t2v_layers T2V_LAYERS]\n",
      "                             [--sent_layers SENT_LAYERS]\n",
      "                             [--moment_layers MOMENT_LAYERS]\n",
      "                             [--dummy_layers DUMMY_LAYERS]\n",
      "                             [--dim_feedforward DIM_FEEDFORWARD]\n",
      "                             [--hidden_dim HIDDEN_DIM]\n",
      "                             [--input_dropout INPUT_DROPOUT]\n",
      "                             [--dropout DROPOUT]\n",
      "                             [--txt_drop_ratio TXT_DROP_RATIO] [--use_txt_pos]\n",
      "                             [--nheads NHEADS] [--num_queries NUM_QUERIES]\n",
      "                             [--num_dummies NUM_DUMMIES]\n",
      "                             [--total_prompts TOTAL_PROMPTS]\n",
      "                             [--num_prompts NUM_PROMPTS] [--pre_norm]\n",
      "                             [--n_input_proj N_INPUT_PROJ]\n",
      "                             [--contrastive_hdim CONTRASTIVE_HDIM]\n",
      "                             [--temperature TEMPERATURE]\n",
      "                             [--saliency_margin SALIENCY_MARGIN]\n",
      "                             [--no_aux_loss] [--span_loss_type {l1,ce}]\n",
      "                             [--contrastive_align_loss]\n",
      "                             [--set_cost_span SET_COST_SPAN]\n",
      "                             [--set_cost_giou SET_COST_GIOU]\n",
      "                             [--set_cost_class SET_COST_CLASS]\n",
      "                             [--lw_saliency LW_SALIENCY] [--lw_wattn LW_WATTN]\n",
      "                             [--lw_ms_align LW_MS_ALIGN]\n",
      "                             [--lw_distill LW_DISTILL]\n",
      "                             [--span_loss_coef SPAN_LOSS_COEF]\n",
      "                             [--giou_loss_coef GIOU_LOSS_COEF]\n",
      "                             [--label_loss_coef LABEL_LOSS_COEF]\n",
      "                             [--eos_coef EOS_COEF]\n",
      "                             [--contrastive_align_loss_coef CONTRASTIVE_ALIGN_LOSS_COEF]\n",
      "                             [--no_sort_results]\n",
      "                             [--max_before_nms MAX_BEFORE_NMS]\n",
      "                             [--max_after_nms MAX_AFTER_NMS]\n",
      "                             [--conf_thd CONF_THD] [--nms_thd NMS_THD]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=\"c:\\Users\\Ashiq and Adnan\\AppData\\Roaming\\jupyter\\runtime\\kernel-v389ffddfb646f99a66f08c7c4e3cd546f89307c7a.json\"\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    best_ckpt_path, eval_split_name, eval_path, debug = start_training()\n",
    "    if not debug:\n",
    "        input_args = [\"--resume\", best_ckpt_path,\n",
    "                      \"--eval_split_name\", eval_split_name,\n",
    "                      \"--eval_path\", eval_path]\n",
    "\n",
    "        import sys\n",
    "        sys.argv[1:] = input_args\n",
    "        logger.info(\"\\n\\n\\nFINISHED TRAINING!!!\")\n",
    "        logger.info(\"Evaluating model at {}\".format(best_ckpt_path))\n",
    "        logger.info(\"Input args {}\".format(sys.argv[1:]))\n",
    "        start_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
